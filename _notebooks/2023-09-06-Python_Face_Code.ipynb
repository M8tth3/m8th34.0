{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "toc: true\n",
    "comments: false\n",
    "layout: post\n",
    "title: Eye Tracking and Facial Recognition with Python\n",
    "tags: [hacks]\n",
    "categories: [CSP, Week 3] \n",
    "---\n",
    "\n",
    "### Finding Faces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face detection processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(small_frame)\n",
    "\n",
    "    # Display the results\n",
    "    for top, right, bottom, left in face_locations:\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Extract the region of the image that contains the face\n",
    "        face_image = frame[top:bottom, left:right]\n",
    "        \n",
    "        face_image = cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)        \n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video Feed: q to quit', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"solid\">\n",
    "\n",
    "### Blurring Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face detection processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(\n",
    "        small_frame)  # , model=\"cnn\"\n",
    "\n",
    "    # Display the results\n",
    "    for top, right, bottom, left in face_locations:\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Extract the region of the image that contains the face\n",
    "        face_image = frame[top:bottom, left:right]\n",
    "\n",
    "        # Blur the face image\n",
    "        face_image = cv2.GaussianBlur(face_image, (99, 99), 30)\n",
    "\n",
    "        # Put the blurred face region back into the frame image\n",
    "        frame[top:bottom, left:right] = face_image\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video Feed: q to quit', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"solid\">\n",
    "\n",
    "### Recognizing Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "obama_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n",
    "\n",
    "# Load a second sample picture and learn how to recognize it.\n",
    "aashray_image = face_recognition.load_image_file(\"aashray.jpg\")\n",
    "aashray_face_encoding = face_recognition.face_encodings(aashray_image)[0]\n",
    "\n",
    "# Load a second sample picture and learn how to recognize it.\n",
    "mortenson_image = face_recognition.load_image_file(\"mortenson.jpg\")\n",
    "mortenson_face_encoding = face_recognition.face_encodings(mortenson_image)[0]\n",
    "\n",
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    obama_face_encoding,\n",
    "    aashray_face_encoding,\n",
    "    mortenson_face_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Barack\",\n",
    "    \"Aashray\",\n",
    "    \"Mr. Mortenson\"\n",
    "]\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(\n",
    "            rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(\n",
    "                known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # # If a match was found in known_face_encodings, just use the first one.\n",
    "            # if True in matches:\n",
    "            #     first_match_index = matches.index(True)\n",
    "            #     name = known_face_names[first_match_index]\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(\n",
    "                known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35),\n",
    "                      (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6),\n",
    "                    font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video Feed: q to quit', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"solid\">\n",
    "\n",
    "### Eye Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import re\n",
    "#import serial\n",
    "\n",
    "# send serial data to arduino (communicate python --> arduino)\n",
    "#arduino = serial.Serial(port = \"COM3\", baudrate = 9600, timeout = .1)\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "scaling = 0.25  # must be a float (X.Y)\n",
    "# the factor to scale image back up by (to display correctly)\n",
    "scaleUp = int(1 / scaling)\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face detection processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=scaling, fy=scaling)\n",
    "\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(small_frame)\n",
    "    face_landmarks_list = face_recognition.face_landmarks(small_frame)\n",
    "\n",
    "    # leftEyeMiddleX = 0\n",
    "    # leftEyeMiddleY = 0\n",
    "\n",
    "    for face_landmarks in face_landmarks_list:\n",
    "        # Find left eye location, store in string\n",
    "        for facial_feature in face_landmarks.keys():\n",
    "            if facial_feature == \"left_eye\":\n",
    "                leftEye = face_landmarks[facial_feature]\n",
    "\n",
    "                # coordinates for top left of eye, bottom right of eye\n",
    "                topLeft = str(leftEye[1])\n",
    "                bottomRight = str(leftEye[4])\n",
    "\n",
    "                # seperate all digits from the list\n",
    "                # find 0th and 1st value (X coord & Y coord respectively)\n",
    "                # convert that string to an int:\n",
    "                # topLeft coordinates\n",
    "                topLeftX = int((re.findall(r'\\d+', topLeft))[0])  # X\n",
    "                topLeftY = int((re.findall(r'\\d+', topLeft))[1])  # Y\n",
    "                # bottomRight coordinates\n",
    "                bottomRightX = int((re.findall(r'\\d+', bottomRight))[0])  # X\n",
    "                bottomRightY = int((re.findall(r'\\d+', bottomRight))[1])  # Y\n",
    "\n",
    "                # find the middle of the two points: (X1 + X2) / 2, (Y1 + Y2) / 2 = (middleX, middleY)\n",
    "                # convert to int since division by 2 can give a value w/ .5\n",
    "                leftEyeMiddleX = int((topLeftX + bottomRightX)/2)  # middle x\n",
    "                leftEyeMiddleY = int((topLeftY + bottomRightY)/2)  # middle y\n",
    "\n",
    "                # send coords over to the arduino as serial data\n",
    "                #arduino.write(str.encode(leftEyeMiddleX))\n",
    "                #arduino.write(str.encode(leftEyeMiddleY))\n",
    "                # convert from int BACK to string and print the coordinates\n",
    "                print(str(leftEyeMiddleX) + \", \" + str(leftEyeMiddleY))\n",
    "\n",
    "                for top, right, bottom, left in face_locations:\n",
    "                    # Scale back up face locations since the frame we detected in was scaled to \"scale\" size\n",
    "                    top *= scaleUp\n",
    "                    right *= scaleUp\n",
    "                    bottom *= scaleUp\n",
    "                    left *= scaleUp\n",
    "\n",
    "                    # Extract the region of the image that contains the face\n",
    "                    face_image = frame[top:bottom, left:right]\n",
    "\n",
    "                    face_image = cv2.rectangle(\n",
    "                        frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "                    # the circles need the coordinates scaled too\n",
    "                    face_image = cv2.circle(\n",
    "                        frame, (leftEyeMiddleX * scaleUp, leftEyeMiddleY * scaleUp), 3, (0, 0, 255), -1)\n",
    "                    face_image = cv2.circle(\n",
    "                        frame, (leftEyeMiddleX * scaleUp, leftEyeMiddleY * scaleUp), 1, (255, 255, 255), -1)\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Eye tracker (q = quit)', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Find all facial features in all the faces in the image\n",
    "face_landmarks_list = face_recognition.face_landmarks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"solid\">\n",
    "\n",
    "### How It Works\n",
    "\n",
    "This code utilizes one-shot deep learning facial recognition technology with an accuracy of 99.38% (Labled Faced in the Wild benchmark).\n",
    "\n",
    "That's quite the mouthful! Before I explain what that all means, I have to explain how facial recognition works. \n",
    "1. The first step is data collection in which one or several images of an individual's face are collected and create the reference dataset. \n",
    "2. Then, you need to do feature extraction in order to capture important facial features. A common technique is the use of a deep neural network or handmade methods. \n",
    "3. After, you use a similarity metric which determines the similartiy between the features extracted from the reference image(s) and the new unseen face image. \n",
    "4. Using these steps, a machine can then recognize someone's face. It's like how humans do it by picking out certain features of a face and associating it with some identiy (like a person's name).\n",
    "\n",
    "Here, we are using one-shot learning which is able to perform recognition with limited examples. In this case, we only give the computer a single image of a person. One-shot learning is designed to recognze someone's face using a single reference image or a very small batch of images without the need for extensive training data.\n",
    "\n",
    "The advantages of one-shot learning include:\n",
    "- Fewer training samples. Useful in scenarios with limited data or if collecting extensive data is impractical\n",
    "- Can adapt quickly to new individuals without need for retraining entire model\n",
    "\n",
    "However, the limitations of one-shot learning are:\n",
    "- May struggle with some variability like facial expressions, lighting conditions, angles, etc\n",
    "- One-shot learning only works if the reference image quality is good. If the quality is not good, it won't work as well\n",
    "- Effective feature extraction and similarity metric techniques can be challenging and has to be fine-tuned\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
